{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# eps for making value a bit greater than 0 later on\n",
    "eps = np.finfo(float).eps\n",
    "from numpy import log2 as log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\jasme\\Downloads\\weather.csv\")\n",
    "df1=pd.read_csv(r\"C:\\Users\\jasme\\Downloads\\weather_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df.iloc[:,:-1]\n",
    "Y_train=df.iloc[:,-1]\n",
    "X_test=df1.iloc[:,:-1]\n",
    "Y_test=df1.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.35714285714285715, 1: 0.6428571428571429}\n"
     ]
    }
   ],
   "source": [
    "#Prior Probabilities [P(yi) for all yi] can be learnt by counting the number of exmples in the training data for each class.\n",
    "import numpy as np\n",
    "train_size=X_train.shape[0]\n",
    "class_priors={}\n",
    "for outcome in np.unique(Y_train):\n",
    "    outcome_count = sum(Y_train == outcome)    \n",
    "    class_priors[outcome] = outcome_count / train_size\n",
    "print(class_priors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing likelihoods\n",
    "features=list(X_train.columns)\n",
    "likelihoods={}\n",
    "for outcome in np.unique(Y_train):\n",
    "    outcome_count = sum(Y_train == outcome)\n",
    "    for feature in features:\n",
    "        for feat_value in np.unique(X_train[feature]):\n",
    "            count=0\n",
    "            for i in range(len(X_train)):\n",
    "                if(X_train[feature][i]==feat_value and Y_train[i]==outcome):\n",
    "                    count=count+1\n",
    "            likelihoods[(feature,feat_value,outcome)]=(count+1)/(outcome_count+(len(features)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Outlook', 'overcast', 0): 0.1111111111111111,\n",
       " ('Outlook', 'rainy', 0): 0.3333333333333333,\n",
       " ('Outlook', 'sunny', 0): 0.4444444444444444,\n",
       " ('Temp', 'cool', 0): 0.2222222222222222,\n",
       " ('Temp', 'hot', 0): 0.2222222222222222,\n",
       " ('Temp', 'hot ', 0): 0.2222222222222222,\n",
       " ('Temp', 'mild', 0): 0.3333333333333333,\n",
       " ('Humidity', '0rmal', 0): 0.2222222222222222,\n",
       " ('Humidity', 'high', 0): 0.5555555555555556,\n",
       " ('Windy', 'strong', 0): 0.4444444444444444,\n",
       " ('Windy', 'weak', 0): 0.3333333333333333,\n",
       " ('Outlook', 'overcast', 1): 0.38461538461538464,\n",
       " ('Outlook', 'rainy', 1): 0.3076923076923077,\n",
       " ('Outlook', 'sunny', 1): 0.23076923076923078,\n",
       " ('Temp', 'cool', 1): 0.3076923076923077,\n",
       " ('Temp', 'hot', 1): 0.23076923076923078,\n",
       " ('Temp', 'hot ', 1): 0.07692307692307693,\n",
       " ('Temp', 'mild', 1): 0.38461538461538464,\n",
       " ('Humidity', '0rmal', 1): 0.5384615384615384,\n",
       " ('Humidity', 'high', 1): 0.3076923076923077,\n",
       " ('Windy', 'strong', 1): 0.3076923076923077,\n",
       " ('Windy', 'weak', 1): 0.5384615384615384}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Computing probability for each feature value given each class label in the test examples\n",
    "a=len(np.unique(Y_train))\n",
    "prob=np.ones((a,len(X_test)),dtype=np.float)\n",
    "for outcome in(np.unique(Y_train)):\n",
    "    outcome_count = sum(Y_train == outcome)\n",
    "    for feature in features:\n",
    "        for i in range(len(X_test)):\n",
    "            if  (feature,X_test[feature][i],outcome) in likelihoods.keys():\n",
    "                prob[idx][i]=prob[outcome][i]*likelihoods[(feature,X_test[feature][i],outcome)]\n",
    "            else:\n",
    "                prob[idx][i]=prob[outcome][i]*(1/(outcome_count+len(features)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiplying probabilities with class prior probabilities\n",
    "for i in range(prob.shape[0]):\n",
    "    prob[i][:]=prob[i][:]*class_priors[i]\n",
    " \n",
    "#Predicting Labels\n",
    "Y_label=np.zeros(len(Y_test))\n",
    "for i in range(len(X_test)):\n",
    "    if (prob[1,i]>=prob[0,i]):\n",
    "        Y_label[i]=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "[[0 1]\n",
      " [0 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jasme\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jasme\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jasme\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(Y_test,Y_label))\n",
    "print(metrics.confusion_matrix(Y_test,Y_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
