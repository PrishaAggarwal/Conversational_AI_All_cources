{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqsalmBmmvxZ"
      },
      "source": [
        "# Converting Corpus to Features\n",
        "\n",
        "![20.png](attachment:20.png)\n",
        "\n",
        "In order to convert corpus to feature matrix, following steps are used:\n",
        "1. Loading your own corpus\n",
        "2. Pre-processing corpus: Normalization, Tokenization, Stop-word Removal, Stemming\n",
        "3. Converting pre-processed corpus to feature matrix (TDM/DTM or TTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6fDkvQBmvxo"
      },
      "source": [
        "# Loading your own corpus\n",
        "User defined corpus can be imported in Python using two methods:\n",
        "1. Using nltk.corpus PlainTextCorpusReader or CategorizedCorpusReader\n",
        "2. Using file method of Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBduVJIcmvxq"
      },
      "source": [
        "# CorpusReader\n",
        "\n",
        "![20.png](attachment:20.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzlDV9pMmvxs"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import PlaintextCorpusReader\n",
        "path='C:/Users/jasme/Desktop/dataset/'\n",
        "dataset=PlaintextCorpusReader(path,'.*')\n",
        "# The same dataset can be downloaded fom following path: 'https://drive.google.com/drive/folders/1zhUOF8X-URYpmB-Zm5PyKjeFKwS7lMFv?usp=sharing/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrJoByRvmvxx",
        "outputId": "b3a69351-af32-4cc2-e6de-3431a2690b66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Data Science is an important field of science .',\n",
              " 'This is an important data science course',\n",
              " 'The cars are driven on the roads .',\n",
              " 'The trucks are driven on the highways']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Converting the dataset to a list (where each string represents a seperate document)\n",
        "corpus=[]\n",
        "for i  in dataset.fileids():\n",
        "    corpus.append(dataset.raw(fileids=i))\n",
        "corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRDTlahYmvx1"
      },
      "source": [
        "# Loading Corpus Using File Method of Python\n",
        "\n",
        "Using files:  \n",
        "File_object=open(r\"File_Name\",\"Access_Mode\")\n",
        "\n",
        "Access Modes :\n",
        "1. Read Only (‘r’)\n",
        "2. Read and Write (‘r+’)\n",
        "3. Write Only (‘w’)\n",
        "4. Write and Read (‘w+’)\n",
        "5. Append Only (‘a’)\n",
        "6. Append and Read (‘a+’)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiyVEtQMmvx4",
        "outputId": "9f081d27-a9e2-48bd-95ef-e29740dc25f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Data Science is an important field of science .',\n",
              " 'This is an important data science course',\n",
              " 'The cars are driven on the roads .',\n",
              " 'The trucks are driven on the highways']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "filenames=os.listdir(path)\n",
        "corpus=[]\n",
        "for i  in range(len(filenames)):\n",
        "    f=open(path+filenames[i],'r')\n",
        "    corpus.append(f.read())\n",
        "    f.close()\n",
        "corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs91uWBfmvx6"
      },
      "source": [
        "# Pre-processing: Step 1: Normalization\n",
        "\n",
        "Normalization in text includes following steps:\n",
        "1. Converting the text into same case (lower, upper, or proper case)\n",
        "2. Removing numbers, special symbols, urls from text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAGjTdcqmvx8",
        "outputId": "e65ba7e5-84db-4c74-d364-6159e3a66c0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['data science is an important field of science .',\n",
              " 'this is an important data science course',\n",
              " 'the cars are driven on the roads .',\n",
              " 'the trucks are driven on the highways']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Converting text to lower case using .lower() method of NLTK\n",
        "lower=[]\n",
        "for i in corpus:\n",
        "    lower.append(' '.join([word.lower() for word in i.split()]))\n",
        "lower"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBZWURaemvx-",
        "outputId": "a9cde217-69f1-46b9-9ed4-4bcfadf7abe5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['data science is an important field of science',\n",
              " 'this is an important data science course',\n",
              " 'the cars are driven on the roads',\n",
              " 'the trucks are driven on the highways']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Removing numbers, special symbols, urls using .isalpha() method of NLTK\n",
        "alpha=[]\n",
        "for i in lower:\n",
        "    alpha.append(' '.join([word for word in i.split() if word.isalpha()]))\n",
        "alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faG45XiQmvyA"
      },
      "source": [
        "# Pre-processing Step 2: Tokenization\n",
        "\n",
        "Tokenization involves converting each document as list of words. It can be done in two ways:\n",
        "1. .split() method of list\n",
        "2. word_tokenize method of nltk.tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOE0j_T-mvyA",
        "outputId": "6fe71756-d6d5-425a-f321-1b4950ae624a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['data', 'science', 'is', 'an', 'important', 'field', 'of', 'science'],\n",
              " ['this', 'is', 'an', 'important', 'data', 'science', 'course'],\n",
              " ['the', 'cars', 'are', 'driven', 'on', 'the', 'roads'],\n",
              " ['the', 'trucks', 'are', 'driven', 'on', 'the', 'highways']]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Tokenization using .split()\n",
        "tokenize=[]\n",
        "for i in alpha:\n",
        "    tokenize.append([word for word in i.split()])\n",
        "tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-FpWSH6mvyC",
        "outputId": "74229ba4-734f-4580-84d5-2eee45c6bbd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['data', 'science', 'is', 'an', 'important', 'field', 'of', 'science'],\n",
              " ['this', 'is', 'an', 'important', 'data', 'science', 'course'],\n",
              " ['the', 'cars', 'are', 'driven', 'on', 'the', 'roads'],\n",
              " ['the', 'trucks', 'are', 'driven', 'on', 'the', 'highways']]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Tokenization using word_tokenize\n",
        "tokenize=[]\n",
        "from nltk.tokenize import word_tokenize\n",
        "for i in alpha:\n",
        "    tokenize.append(word_tokenize(i))\n",
        "tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmA4VllemvyD"
      },
      "source": [
        "# Pre-processing Step 3: Stop-word Removal\n",
        "A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that does not have any linguistic importance in NLP applications\n",
        "\n",
        "NLTK(Natural Language Toolkit) in python has a list of stopwords stored in stopwords corpus in 16 different languages.\n",
        "\n",
        "The name of fields is the name of language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdWbtzxymvyE",
        "outputId": "435027df-76f7-4a03-88c1-3dfd18fade97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['data', 'science', 'important', 'field', 'science'],\n",
              " ['important', 'data', 'science', 'course'],\n",
              " ['cars', 'driven', 'roads'],\n",
              " ['trucks', 'driven', 'highways']]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import *\n",
        "stopword=nltk.corpus.stopwords.words('english') #stopword will contain list of all stopwords of english language\n",
        "no_stop=[]\n",
        "for i in tokenize:\n",
        "    no_stop.append([word for word in i if word not in stopword])\n",
        "no_stop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtctY6y6mvyF"
      },
      "source": [
        "# Pre-processing Step 4: Stemming\n",
        "\n",
        "Stemming is a process that maps variant word forms to their base forms (play, plays, playing, played )\n",
        "\n",
        "nltk.stem has number of stemming algorithms named as \"PorterStemmer\", \"LancasterStemmer\", etc. These algorithms accepts the list of tokenized word and stems it into root word.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLx8-25cmvyG",
        "outputId": "ea436ce6-ba5d-45e6-d171-564336871635"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'depart'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Stemming Example\n",
        "from nltk.stem import PorterStemmer #Importing porter stemmer class\n",
        "ps=PorterStemmer() #Creating an object of PorterStemmer Class\n",
        "ps.stem('departed') #stemming a word using .stem method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bn372sAUmvyH",
        "outputId": "cdb33689-6002-4c4a-d8e9-e524daf8fe5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['data scienc import field scienc',\n",
              " 'import data scienc cours',\n",
              " 'car driven road',\n",
              " 'truck driven highway']"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Stemming the corpus\n",
        "final=[] #will contain final pre-processed documents\n",
        "from nltk.stem import PorterStemmer\n",
        "ps=PorterStemmer()\n",
        "for i in no_stop:\n",
        "    final.append(' '.join([ps.stem(word) for word in i]))\n",
        "final"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}