{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtgATsqS8Xu7l0zJyunoba",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrishaAggarwal/Conversational_AI_All_cources/blob/main/Accelerated_Data_Science/Lab/My_files_Colab/Convo_AI_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_Q2Sl1N_BS-"
      },
      "outputs": [],
      "source": [
        "#deep neural network implementation\n",
        "#first we are implementing we apply forward propagation\n",
        "#binary wala implement kr rhe since we only know sigmoid activation function\n",
        "#we need in order features,examples acc to our derivation in class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "yquubMlH_MFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "dataset=load_breast_cancer()\n",
        "X=dataset.data\n",
        "Y=dataset.target"
      ],
      "metadata": {
        "id": "BGRRCDIV_PSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape #hence it has 369 examples and 30 features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qs1K07rZ_jeT",
        "outputId": "dd92198a-9977-4f4b-f300-c7848e318745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "DowUKcHp_pPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=X_train.T\n",
        "X_test=X_test.T\n",
        "Y_train=Y_train.reshape(1,-1) #converted into one row\n",
        "Y_test=Y_test.reshape(1,-1)"
      ],
      "metadata": {
        "id": "A5aesMUeAH31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sabse phele define architecture of neural network\n",
        "#kitni layers and kitne neural networks..chahe yeh kaam hyperprameters ka kaam hai\n",
        "#tensorflow mein we have library mein jo tune krke de deti hai\n",
        "#u might take diff number of layers here\n",
        "#in tensorflow and pytorch bas yahi define krna hota hai architecture and u wont know forward ya backward propogation chl rha...no trainable parameters in  this layer\n",
        "nn_arch=[\n",
        "    {'layers_unit':30,'activation':'none'},#input layer doesnt do any activation so none #input layer has something to be trained\n",
        "    {'layers_unit':5,'activation':'relu'},\n",
        "    {'layers_unit':4,'activation':'relu'}, #4*5+4=24 trianable paramaters hai iske\n",
        "    {'layers_unit':3,'activation':'relu'},\n",
        "    {'layers_unit':1,'activation':'sigmoid'} #1 neuron since sigmoid .... agar 10 classes then 10 neurons with softmax function\n",
        "]"
      ],
      "metadata": {
        "id": "nIK86c2gASQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#aab har layer of weight bias define krenge\n",
        "def initialize_parameters(nn_arch,seed_value):\n",
        "  #we give seed value so that we give seed everytime and same parameters will be generated\n",
        "  np.random.seed(seed_value)\n",
        "  #iss nn_arch se it will know number of layers and functions\n",
        "  parameters={}\n",
        "  num_layers=len(nn_arch) #u will start from 1 to 5 and 5 will not be included\n",
        "  #for 0th layer we dont need to define and 5th will be included in 1 to 5\n",
        "  for l in range(1,num_layers):\n",
        "    parameters['W'+str(l)]=np.random.randn(nn_arch[l]['layers_unit'],nn_arch[l-1]['layers_unit'])*0.01 #0.01 se multiplied since relu function suffers from gradient exploding problem\n",
        "    #'w'  is weight of that layer and we have initialized the shape of weight as n,n-1\n",
        "    #u cant initialize here to zero like in machine learning since varna sab neurons same ho jayenge i.e zero\n",
        "    #sab neurons will become zero and activation will become zero and then non linear khtm\n",
        "    #bias can be zero  initialize but not neurons\n",
        "    #normally distributed inialization krni hai values called zavier initilization\n",
        "    parameters['B'+str(l)]=np.zeros((nn_arch[l]['layers_unit'],1)) #bias initialized for lth layer with dimension l,1\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "Scn_6oPXBzJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters=initialize_parameters(nn_arch,3)"
      ],
      "metadata": {
        "id": "45eZSnm_Ev23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters['W1'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MjntEwVE04_",
        "outputId": "89b41fea-7704-439f-eee3-f32190cc5055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(z):\n",
        "  return np.maximum(0,z)\n",
        "  #agar z value is negative then gives 0 and otherwise same value\n",
        "\n",
        "def sigmoid(z):\n",
        "  return 1/(1+np.exp(-z))\n",
        "#take any function but define that mathematical function here"
      ],
      "metadata": {
        "id": "7KiN8kc3H6Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(nn_arch,x,paramters): #kitni layers hai voh arch se pta lagega..intial paramters passing\n",
        "  #x is input feature set and yeh given hota hai and calculate nhi hota so cache mein already yeh save kr lenge\n",
        "  forward_cache={} #stores everything we need for backward from forward propagtion\n",
        "  num_layers=len(nn_arch)\n",
        "  forward_cache['A0']=x\n",
        "  for l in range(1,num_layers):\n",
        "    #we are seeing what are weight bias and activation function of layers\n",
        "    W=parameters['W'+str(l)]\n",
        "    B=paramters['B'+str(l)]\n",
        "    activation=nn_arch[l]['activation']\n",
        "    #now we have everything we need and we will compute the z value\n",
        "    forward_cache['Z'+str(l)]=np.dot(W,forward_cache['A'+str(l-1)])+B #in general its W(l)A(l-1)+B(l)...this formula is only valid when dimensions are n0,n\n",
        "    #relu mein less than 0 is 0 and more then 0 as it is...used more for images since negative values ko dead krna hai\n",
        "    if activation=='relu':\n",
        "      forward_cache['A'+str(l)]=relu(forward_cache['Z'+str(l)])\n",
        "    elif activation=='sigmoid':\n",
        "      forward_cache['A'+str(l)]=sigmoid(forward_cache['Z'+str(l)])\n",
        "    #it should also return final calculated value since we have to find cost also jiske basis par hyperparamters are tuned toh predicted value bhi dekhni hai\n",
        "    AL=forward_cache['A'+str(num_layers-1)] #last layer ki value is predicted value\n",
        "    return AL,forward_cache\n",
        "#we have just computed Z,A and osko save kr liya"
      ],
      "metadata": {
        "id": "oNIGkDbrFuTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AL,forward_cache=forward_propagation(nn_arch,X_train,parameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "OYE34kc6JNoU",
        "outputId": "ab722873-b9eb-4741-fb5d-22bad1a1454c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'A4'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1919852352.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforward_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_arch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2612250453.py\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(nn_arch, x, paramters)\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mforward_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Z'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#it should also return final calculated value since we have to find cost also jiske basis par hyperparamters are tuned toh predicted value bhi dekhni hai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mAL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforward_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#last layer ki value is predicted value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforward_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#we have just computed Z,A and osko save kr liya\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'A4'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters['B1'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0bYyLkeJYXP",
        "outputId": "80e5dc49-f832-428a-f748-e12d59b0de2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(y,AL):\n",
        "  #we give actual value y and AL is predicted value\n",
        "  n=y.shape[1]\n",
        "  cost=(-1/n)*np.sum((y*np.log(AL+1e-8)+(1-y)*np.log(1-AL+1e-8))) #binary cross entropy cost function\n",
        "  #added a very small value inside log i.e 1e-8 taki log 0 aake infinity na aa jaye\n",
        "  #hyperparamter tuning ke time sometimes log 0 aa jata hai so osko avoid krne ke liye we did this\n",
        "  return cost"
      ],
      "metadata": {
        "id": "bJM8oxpZKLbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_cost(Y_train,AL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "nkJVQpvYM6Gw",
        "outputId": "f86ee593-08e5-42f8-f6a9-9aab191bfd31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AL' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1008788747.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompute_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'AL' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lets start backward propagation\n",
        "#we have compute dA(l) khud compute krke dena padega of  last layer\n",
        "#aise hi we have derived for all other parameters in backward propogation\n",
        "#aise hi we find dw(l) and db(l) nikalte hai"
      ],
      "metadata": {
        "id": "haxlX2oxK7mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_backward(dA_prev,Z): #g'(z) is derivation of sigmoid function w.r.t z\n",
        "  #when g is diff then g' i.e activation function alag hoga in backward\n",
        "  dact=sigmoid(Z)*(1-sigmoid(Z)) #dact is derivative of activation function\n",
        "  return dA_prev*dact"
      ],
      "metadata": {
        "id": "c5r7xgdtMhj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_backward(dA_prev,Z):\n",
        "  #0 par bhi we take 0 kyuki 0 neuron aane ki probability is very less\n",
        "  dA_prev=np.array(dA_prev,copy=True) #pichla dA aage chahiye hoga and hence voh erase na ho\n",
        "  dA_prev[Z<0]=0\n",
        "  #when z is greater than 1 then it will get multiplied by 1 i.e da prev vhi rahega but 0 wale case mein it becomes zero so\n",
        "  return dA_prev"
      ],
      "metadata": {
        "id": "aj0CjKtlNSax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation(nn_arch,y,AL,forward_cache,parameters): #we need y for dA phele wala to calculate, need w from parameters, and activation functions from forward cache\n",
        "  grads={} #gradients #loss function ka derivative w.r.t wt and bias store krenge ismein\n",
        "  num_layers=len(nn_arch)\n",
        "  dA_prev=(y-AL)/(AL*(1-AL)) #da_prev pheli baari mein hum compute krke denge..binary classification mein da_prev ka yeh formula hai varna for multiclass its diff\n",
        "  n=y.shape[1]\n",
        "  for l in reversed(range(1,num_layers)):\n",
        "    #backward ulti chlti hai 4,3,2,1 chlegi\n",
        "    #har layer ke 4 values nikalni hai and onmein jo cheeze use hogi voh phele nikalo\n",
        "    z_curr=forward_cache['Z'+str(l)]\n",
        "    A_prev=forward_cache['A'+str(l-1)]\n",
        "    W_curr=parameters['W'+str(l)]\n",
        "    activation=nn_arch[l]['activation']\n",
        "    if activation=='relu':\n",
        "      dZ=(1/n)*relu_backward(dA_prev,z_curr) #cost mein 1/n hota hai so derivation mein bhi 1/n aayega\n",
        "      grads['dW'+str(l)]=np.dot(dZ,A_prev.T)\n",
        "      grads['dB'+str(l)]=np.sum(dZ,axis=1,keepdims=True) #agar addition kr rhe then column is return ho keepdims=True\n",
        "      dA_prev=np.dot(W_curr.T,dZ)\n",
        "    elif activation=='sigmoid':\n",
        "      dZ=(1/n)*sigmoid_backward(dA_prev,z_curr)\n",
        "      grads['dW'+str(l)]=np.dot(dZ,A_prev.T)\n",
        "      grads['dB'+str(l)]=np.sum(dZ,axis=1,keepdims=True)\n",
        "      dA_prev=np.dot(W_curr.T,dZ)\n",
        "    return grads"
      ],
      "metadata": {
        "id": "V_r6UDX6OC6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grads=backward_propagation(nn_arch,Y_train,AL,forward_cache,parameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "fSD5_xS7O_Jt",
        "outputId": "48a2247e-0ca6-498b-a4f8-8970c266a39a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AL' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3223466646.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_arch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforward_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'AL' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(paramters,grads,lr,nn_arch): #optimizing techniques using\n",
        "#we are using here gradient descent\n",
        "  for l in range(1,len(nn_arch)):\n",
        "    parameters['W'+str(l)]=parameters['W'+str(l)]-lr*grads['dW'+str(l)]\n",
        "    parameters['B'+str(l)]=parameters['B'+str(l)]-lr*grads['dB'+str(l)]\n",
        "  return parameters\n",
        "  #gradient descent neural networks mein not used since zig zag curve ban jata hai and hamare parameters ache se optimize nhi hote"
      ],
      "metadata": {
        "id": "CLajrgJlRS4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "update_parameters(parameters,grads,0.01,nn_arch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "OqMXAR7gRekR",
        "outputId": "415af4cb-3767-49c4-e7cb-888d15465567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'grads' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2820178703.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnn_arch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'grads' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def training(x,y,nn_arch,lr,iterations): #ismein yeh sab ekathe krenge\n",
        "#model.compile yahi training wala part kr rha hai\n",
        "  parameters=initialize_parameters(nn_arch,3) #har layer ka weight and bias initialized mil gya\n",
        "  costs=[] #list of cost\n",
        "  for i in range(iterations): #iterations is hyperparameter\n",
        "    AL,forward_cache=forward_propagation(nn_arch,x,parameters) #phele z and activation compute hogi using forward propogation for each iterations\n",
        "    cost=compute_cost(y,AL) #current values of weight and bias se cost computed\n",
        "    grads=backward_propagation(nn_arch,y,AL,forward_cache,parameters) #gradients computed\n",
        "    parameters=update_parameters(parameters,grads,lr,nn_arch)\n",
        "    #after every 100 iterations print the cost to check it regularly\n",
        "    if(i%100==0):\n",
        "      print('cost after '+str(i)+' iterations is '+str(cost))\n",
        "      costs.append(cost) #plot krenge yeh cost and osse hyperparameter tuning krenge and also see if curve is converging or not\n",
        "  return costs,parameters,AL #we return predicted values since testing ke time predicted value chahiye hogi"
      ],
      "metadata": {
        "id": "zpgdkeQtSZzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "costs,parameters,predicted_values=training(X_train,Y_train,nn_arch,0.01,10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "zQKn45eSTmMG",
        "outputId": "3ceec903-f079-4d98-9334-90023adab5ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'A4'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2586580074.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnn_arch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#cost badh rhi mtlb learning rate jiyada hai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3843252093.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(x, y, nn_arch, lr, iterations)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mcosts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#list of cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#iterations is hyperparameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforward_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_arch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#phele z and activation compute hogi using forward propogation for each iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mcost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#current values of weight and bias se cost computed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mgrads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_arch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforward_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#gradients computed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2612250453.py\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(nn_arch, x, paramters)\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mforward_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Z'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#it should also return final calculated value since we have to find cost also jiske basis par hyperparamters are tuned toh predicted value bhi dekhni hai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mAL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforward_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#last layer ki value is predicted value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforward_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#we have just computed Z,A and osko save kr liya\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'A4'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot this cost\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(costs)\n",
        "#converging costs but zig zag curves coming\n",
        "#this is why gradient descent not used in deep learning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "y-fJjVsJTzyX",
        "outputId": "b9e844aa-8d69-4f67-ecf0-9f47abd274c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'costs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-30980924.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#plot this cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#converging costs but zig zag curves coming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#this is why gradient descent not used in deep learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'costs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we have weights after training and X _test also\n",
        "#forward propagation se we find predicted value\n",
        "#akhri neuron ki value is our ans\n",
        "Y_predicted,forward_cache=forward_propagation(nn_arch,X_test,parameters)\n",
        "#testing ke time we dont need forward cache but we wrote since forward wala function it returns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "S-QQm3OQbjqh",
        "outputId": "cbd81eaa-f71f-4c81-9212-bbf68954263a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'A4'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-821659152.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#forward propagation se we find predicted value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#akhri neuron ki value is our ans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mY_predicted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforward_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_arch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2612250453.py\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(nn_arch, x, paramters)\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mforward_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Z'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#it should also return final calculated value since we have to find cost also jiske basis par hyperparamters are tuned toh predicted value bhi dekhni hai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mAL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforward_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#last layer ki value is predicted value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforward_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#we have just computed Z,A and osko save kr liya\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'A4'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#values are between 0 to 1 since sigmoid values hai\n",
        "Y_predicted[Y_predicted<=0.5]=0\n",
        "Y_predicted[Y_predicted>0.5]=1\n",
        "Y_predicted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "2HwhocmncMPs",
        "outputId": "ae274251-6784-4e8d-d6df-09ba64f1a029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Y_predicted' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3803600723.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#values are between 0 to 1 since sigmoid values hai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mY_predicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mY_predicted\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mY_predicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mY_predicted\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mY_predicted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Y_predicted' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(Y_predicted==Y_test)/len(Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "OoifqVHzcgw8",
        "outputId": "7d25886f-a243-40ec-fb09-42dde6076aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Y_predicted' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1494510272.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_predicted\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Y_predicted' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(Y_predicted.reshape(-1,1),Y_test)\n",
        "#in built functions demand that column vector mein ho feature so we reshape it"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "fblsrCi_cnuU",
        "outputId": "c4c10a02-6fcf-4e7f-f75b-8183c0b8fd17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Y_predicted' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2168120494.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_predicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#in built functions demand that column vector mein ho feature so we reshape it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Y_predicted' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model=[\n",
        "    'accuracy':-np.inf, #we need accuracy high so anything more than -inf is better\n",
        "    'cost':np.inf, #we need minimum cost so highest se everything is better\n",
        "    'lr':None,\n",
        "    'iteration':None,\n",
        "    'predicted_value':None,\n",
        "    'parameters':None\n",
        "    ]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "5xUZ53A1dter",
        "outputId": "4cd626d7-c67b-4f91-f11c-b7d161ba1e7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-613588809.py, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-613588809.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    'accuracy':-np.inf, #we need accuracy high so anything more than -inf is better\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#HYPERPARAMETER TUNING\n",
        "#Basically grid search hi hoti hai\n",
        "epochs=[100,1000,10000] #we take a list of epochs\n",
        "alphas=[1e-4,1e-3,1e-2,1e-1]\n",
        "for iteration in epochs:\n",
        "  for lr in alphas:\n",
        "    #aab har combination par we call training function\n",
        "    costs,parameters,AL=training(X_train,Y_train,nn_arch,lr,iteration) #ismein use krte vakat humne cost band krdi since baar baar printing\n",
        "    #aab best model kaise nikalenge ?\n",
        "    #upar ek best model ki dictionary we take with accuracy and cost ke basis we update best model\n",
        "    Y_predicted,_=forward_propagation(nn_arch,X_test,parameters) #trainign se best parameters nikalkr y ko predict kra\n",
        "    Y_predicted[Y_predicted<=0.5]=0\n",
        "    Y_predicted[Y_predicted>0.5]=1\n",
        "    acc=accuracy_score(Y_predicted.reshape(-1,1),Y_test)\n",
        "    if (acc>best_model['accuracy']) or (acc==best_model['accuracy'] and costs[-1]<best_model['cost']):\n",
        "      #[-1]costs mein last cost value jo best cost nikalkr aayi hai\n",
        "      best_model.update({\n",
        "          'accuracy':acc,\n",
        "          'cost':costs[-1],\n",
        "          'lr':lr,\n",
        "          'iteration':iteration,\n",
        "          'predicted_value':AL,\n",
        "          'parameters':parameters\n",
        "       })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "w9sY8xzdc4-G",
        "outputId": "d7aac7ae-9220-4f9b-e72d-d7dfcf75e946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'A4'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-780433054.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#aab har combination par we call training function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcosts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnn_arch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#ismein use krte vakat humne cost band krdi since baar baar printing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m#aab best model kaise nikalenge ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#upar ek best model ki dictionary we take with accuracy and cost ke basis we update best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3843252093.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(x, y, nn_arch, lr, iterations)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mcosts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#list of cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#iterations is hyperparameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforward_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_arch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#phele z and activation compute hogi using forward propogation for each iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mcost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#current values of weight and bias se cost computed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mgrads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_arch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforward_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#gradients computed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2612250453.py\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(nn_arch, x, paramters)\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mforward_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Z'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#it should also return final calculated value since we have to find cost also jiske basis par hyperparamters are tuned toh predicted value bhi dekhni hai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mAL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforward_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#last layer ki value is predicted value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforward_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#we have just computed Z,A and osko save kr liya\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'A4'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "Cd3I2u44fcka",
        "outputId": "0cb6f7e5-d5f4-45f8-fc83-3b140e623b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'best_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2074715960.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'best_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fAN29Qjbf8t8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}