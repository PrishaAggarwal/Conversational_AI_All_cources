{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9034770f-c9ef-4057-bbc0-1c427c3d9060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "385009fa-49bd-4275-881a-ed66e65e8fb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1186210362.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    path =\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#method 1 to load the dataset\n",
    "path = \" \"\n",
    "corpus = PlainTextCorpusReader(path,\".*\") #tool that loads plain text files into NLTK, and gives you easy access to words, sentences, and file lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c18a18-f861-4ec4-9277-08918b571d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#method 2 to load the dataset\n",
    "docs=[]\n",
    "for doc_name in filenames: # loop through each item in filename\n",
    "    docs.append(corpus) #appending entire corpus object into docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4334da2f-edf8-4315-9c5f-e4ff5f3e23b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf17cffa-786e-4af1-b1a0-fc96124116a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization\n",
    "normalize_tok = []\n",
    "for doc in docs:\n",
    "    normalize_tok.append([word.lower() for word in doc.split() if word.isalpha()]) #.split mein andar kuch nhi toh jaha space udhr split\n",
    "    #toh agar sentence ke end period(.) aayega voh without space dalta with last word\n",
    "    #toh last word har sentence ka removed beacuse its with period\n",
    "    #alpha words are a-z and A-Z \n",
    "    #toh to solve this we use regulization..next sem\n",
    "    #we make regular pattern "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3718bb8b-3420-4e8f-bc03-a65535e98c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb73782-e7fb-4b9b-9cdd-6bcd5b1bfc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing special symbols\n",
    "normalize_tok1=[]\n",
    "import re #library for regular expression\n",
    "for doc in docs: #sub means substitue and here we have to give a pattern\n",
    "    #if we write ^ yeh sign then whatever pattern given osse match nhi krta it must be removed\n",
    "    # \\s means blind space\n",
    "    #' ' means whatever pattern given osse doesnt match then null character se replace \n",
    "    #we have to check this in doc so written doc\n",
    "    norm=re.sub('[^a-zA-Z\\s]',' ',doc)\n",
    "    #english mein u can use split where space par new word starts but some other language mein like arabic mein space se word doesnt start\n",
    "    #word_tokenization is trained in many languages toh yeh har word ko split kr dega\n",
    "    normalize_tok1.append([word.lower() for word in ntlk.word_tokenization(norm)]) #norm is upar se output and osmein we want split words isliye writing that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c6e166-76a1-456b-87e4-bc57efeccdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stop words\n",
    "nostop =[]\n",
    "ntlk.corpus.stopwords.words() #corpus mein se stopwords chahiye and osmein only words needed in english..agar words ke andar english nhi likha then har language ke stopwords aa jayenge\n",
    "for doc in normalize_tok1: #each list is a doc in normalize_tok\n",
    "    #picking up each doc in upar wala jo generate kra\n",
    "    #osmein lists hai ospar we will work\n",
    "    nostop.append([word for word in doc if word not in stopwords_list]) #word for word is inline function..isko by for loop u can also do\n",
    "    #yeh word by word sab words ko dekhega\n",
    "nostop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b1a542-c403-46b4-92d3-44c0a6119fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#morphological analysis : map varient form to base form like plays to play\n",
    "#stemming and lametization 2 methods\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PosterStemmer() #make an object of class\n",
    "final =[]\n",
    "for doc in nostop:\n",
    "    #nostop because yahi last preprocessed class hai\n",
    "    final.append([ps.stem(word) for word in doc]) #stem is the method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbe36fd-2974-4186-99e1-8016a6a51745",
   "metadata": {},
   "outputs": [],
   "source": [
    "final #iski output may not make meaningfull word because only prefixs removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c94f9f8-4c48-4514-99e9-547dc1bec3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lantaster stammmer \n",
    "#more regrerous and gives more accurate results but still porter stemmer more famous\n",
    "#because more languages in porter \n",
    "final1 =[]\n",
    "from nltk.stem import LancasterStemmer\n",
    "stopwords_list = stopwords.words('english')\n",
    "ls=LancasterStemmer()\n",
    "for doc in docs:\n",
    "    norm=re.sub('[^a-zA-Z]',' ',docs)\n",
    "    norm_tok = [word.lower() for word in word in norm.split()]\n",
    "    nostop = [word for word in norm_tok if word not in stopwords_list]\n",
    "    final1.append(' '.join[ls.stem(word) for word in nostop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d71cef1f-f89e-4bb8-b382-0e4b3e81c99a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'WordNetLemmatizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#lemmatization\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# changes writing below\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mWordNetLemmatizer\u001b[39;00m\n\u001b[0;32m      4\u001b[0m wn \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'WordNetLemmatizer'"
     ]
    }
   ],
   "source": [
    "#lemmatization\n",
    "# changes writing below\n",
    "import WordNetLemmatizer\n",
    "wn = WordNetLemmatizer()\n",
    "#last line\n",
    "final1.append(' '.join[wn.lemmatize(word) for word in nostop]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff15ba05-478a-4174-bc72-1a5efab23896",
   "metadata": {},
   "source": [
    "Now we have preprocssed dataset/corpus and now we will create TDM from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a1e9008-bfcc-4815-8abd-3e7c4df2ad03",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#dictionary defining called doccount..that maintains count of docs\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#inner dictionary having word in each doc\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#here the corpus is small toh phele sab words nikalkr unique words nhi dekhne padenge\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#agar word is already present in disctionary then ignore that word and else if thw word not present then add that word in dcitionayr\u001b[39;00m\n\u001b[0;32m      6\u001b[0m doc_count \u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(final)): \u001b[38;5;66;03m#final is last preprocced list\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     word_count \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'final' is not defined"
     ]
    }
   ],
   "source": [
    "#dictionary defining called doccount..that maintains count of docs\n",
    "#inner dictionary having word in each doc\n",
    "#here the corpus is small toh phele sab words nikalkr unique words nhi dekhne padenge\n",
    "#agar word is already present in disctionary then ignore that word and else if thw word not present then add that word in dcitionayr\n",
    "\n",
    "doc_count =[]\n",
    "for i in range(len(final)): #final is last preprocced list\n",
    "    word_count = {}\n",
    "    for word in final[i].split(): #split each sentence into words by spaces and hence looping one word at one time\n",
    "        #check if the word is in ith document\n",
    "        if word not in word_count.keys(): #.keys() returns all keys i.e words already stored\n",
    "            word_count[word] = 1 #values will be binary toh chahe jitni marzi baar aaye we will leave it binary\n",
    "        #all words checked then update word count\n",
    "        doc_count[filenames[i]] = word_count #all names in filenames toh filenames mein hi storing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b93cf88-21d9-4bc5-a4ba-ba18385afce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm_binary_scratch = pd.Dataframe(doc_count) #inner keys are row labels and outer keys columns labels by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908c279c-aef5-4ab7-85dd-e65cbd1149a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm_binary_scratch.fillna(0,inplace=True)\n",
    "#NAN aa rha toh osko o fill krne ke liye we did fillna..inplace=true means permanent hoga and not create a copy and do it in same dataframe\n",
    "tdm_binary_sratch=tdm_binary_scratch.sort_index(ascending=True)\n",
    "#in standard method the dictionary is sorted ..all the words are sorted\n",
    "#u can do another preprocessing .sort_index in ascending or descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3406cda-ad58-447c-adeb-6a7b5c1a216d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#count vectorize and ek hota tdf-idf vectorize\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#u can see the documentation of count vectorize\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#oss function se directly preprocessing hogi\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#max_id and min...hume high frequency and low frequency words nhi itne imp  nhi hote\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#toh we can set if the word in appearing minimum 5 docs and maximum 100 docs ..aaise\u001b[39;00m\n\u001b[0;32m     14\u001b[0m cv \u001b[38;5;241m=\u001b[39m CountVectorizer(binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#count vectorize and ek hota tdf-idf vectorize\n",
    "#u can see the documentation of count vectorize\n",
    "#oss function se directly preprocessing hogi\n",
    "#os function mein see each parameter of function does what\n",
    "#this makes dtm..u can always convert it into tdm...... X.toarray().T\n",
    "\n",
    "#max_features ...max kitne features krne hia\n",
    "#vocabulary mein  u can give list of vocabulary jo dekhne hai\n",
    "#by default binary tdm nhi banata...binary wala feature = true krdo then yeh binary tdm dega\n",
    "#max_id and min...hume high frequency and low frequency words nhi itne imp  nhi hote\n",
    "#toh we can set if the word in appearing minimum 5 docs and maximum 100 docs ..aaise\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "X = fit_transform(final) #fit means learning the parameters of model...features are words of document whose frequency is 0/1\n",
    "#X learn kr liya in form of compressed sparse format...i.e only non-zero entries stored and faltu mein zero wali values not stored/learned\n",
    "print(X)\n",
    "cv.get_features_names() #learning the names of features..i.e words\n",
    "cv.get_stop_words() #since we have removed stop words toh yeh kuch return nhi krega\n",
    "\n",
    "#transform means after learning , it generates the output for model whatebver is requriment \n",
    "#linear regression is predict because osmein beta learn krke u have find the values..ismein akela fit hota hai\n",
    "\n",
    "tdm_binary_inbuilt=pd.Dataframe(X.toarray().T,index=cv.get_feature_names(),columns=filenames) #converted dtm to tdm\n",
    "#index mein humne words de diye jo get feature mein stored hai and filenames mein docs saved toh voh columns name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a66f3a6-7df5-4b3a-89f3-2b71021f93e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm_binary_inbuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc01d1f7-d144-44a2-b522-6557948f24c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual term frequenecy wala\n",
    "doc_count1 ={}\n",
    "for i in range (len(final)):\n",
    "    word_count1 ={} #number of words in each doc\n",
    "    for word in final[i].split(): #ith doc ko uthaya and osko split kiya and osmein se ek ek krke sab words uthaya\n",
    "        if word not in word_count1.keys(): #agar voh word phele baar aa rha then oski entry dalenge and osko zero put krege\n",
    "            word_count1[word]=0\n",
    "        word_count1[word] += 1 #agar word 2nd time aaya then if condition not satisfied and oska word count incremented by 1\n",
    "    doc_count1[filenames] = word_count1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2795c08-fa7c-49c4-96a3-a908e04901eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_count1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9246318a-7c45-4b8c-80c1-e0943b5fe871",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tdm_tf_scratch \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataframe(doc_count1)\n\u001b[0;32m      2\u001b[0m tdm_tf_scratch\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m,inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "tdm_tf_scratch = pd.Dataframe(doc_count1)\n",
    "tdm_tf_scratch.fillna(0,inplace=True)\n",
    "tdm_tf_sratch = tdm_tf_scratch.sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6d0f8f-f903-40a3-9cad-f3286d73125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm_tf_scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75bee687-2aba-4349-af80-017f6b376082",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#for inbuild banana hia\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#actual term frequnecy\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[0;32m      4\u001b[0m cv1 \u001b[38;5;241m=\u001b[39m CountVectorizer()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "#for inbuild banana hia\n",
    "#actual term frequnecy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv1 = CountVectorizer()\n",
    "X1=cv1.fit_tranform(final)\n",
    "tdm_tf_inbuilt=pd.DataFrame(X1.toarray().T,index=cv1.get_feature_names(),columns=filenames)\n",
    "tdm_tf_inbuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f714e2f5-fb82-4f1b-99df-344b16dd7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 3 : length normalization\n",
    "tdm_len_scratch=tdm_tf_scratch.iloc[:,:].div(tdm_tf_scratch.sum(axis=0),axis=1) #jo term frequnecy we made from scratch osmein har value ko liya and divide them i.e .div() method by sum of frequency of rows\n",
    "#jitni terms aa rhi hai ek doc mein oska sum lena hai and division column wise krenge\n",
    "#har doc ka term frequency aaya total and we will the ans with each column\n",
    "#after excution hamari har value normlize ho gyi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7771d3ad-0c68-48cf-ad8c-b42fc481d25d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1379137964.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[13], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    TdidVectorizer(\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "#using inbuilt function\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#hume bas term frequency chahiye and not idf toh osko false kr denge\n",
    "#use idf parameter false krdo\n",
    "#u can check its documention as well\n",
    "tf=TdidVectorizer(use_idf=False,norm='l1') #bas do normalization kr sakte ho..l0 or l1\n",
    "X2 = tf.fit_transform(final)\n",
    "tf_len_inbuilt=pd.DataFrame(X2.toarray().T,index=tf.get_feature_names(),columns=filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f75e0-f3c7-44e9-9cef-278ab5ae69c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_len_inbuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5130dd7-91d6-432c-a052-8127c84327db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next method\n",
    "#max normalization\n",
    "tdm_max_scratch=tdm_tf_scratch.iloc[:,:].div(tdm_tf_scratch.max(axis=0),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9677d5b4-fbc6-4514-8ec8-bf188cf27b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm_max_scratch #no inbuild method for max normakiuzation in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a190752e-d14e-4801-9207-7fa1fd373ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5th method\n",
    "#log normalization\n",
    "tdm_log_scratch=np.log(1+tdm_tf_scratch) #term frequenecy might be zero toh isliye 1 added since log 0 is not defined\n",
    "#no build method in skleanr for it as well\n",
    "#frequencies very hgih in big corpus toh 1 add krne se farak nhi padhta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b95069-8e16-4057-bdf8-e1f8ce9ed6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6th method\n",
    "#tf-idf\n",
    "#most imp\n",
    "#out tf is created already and oske 5 varients hai and koi bhi varient can be used\n",
    "#idf is corpus level feature\n",
    "unique_terms = tdm_tf_scratch.index #hamari unqiue terms are tdm ke rows labels\n",
    "unique_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb91142b-7f34-4ebc-86c2-8775f5b960e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#single dictionaru is enough since it is not document level feature\n",
    "idf_count = {}\n",
    "for term in unique_terms:\n",
    "    idf_count[term]=0\n",
    "    for doc in final: #checking all docs..ek doc mein term agar hai then increase the count of idf\n",
    "        if term in doc.split():\n",
    "            idf_count[term] += 1\n",
    "    #after checking all docs then find idf value of that term\n",
    "    idf_count[term]=np.log(len(final)/idf_count[term])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80014dec-2f92-492f-8d2b-8df6eda858df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sab unique terms ki idf nikal jayega\n",
    "idf_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce53dcc0-035d-47df-83c3-5d614cdbf262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making dataframe after getting the idf score\n",
    "pd.DataFrame(pd.Series(idf_count),columns=['IDF_SCORE']) \n",
    "#direct single dictionary wont be converted so phele osko series banana padega and then convert hoga\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb2db44-5230-42d4-85b3-ca891d1b7eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now finally finding tf-idf\n",
    "#term frequency ka we can take any varient but mostly length wala we take\n",
    "#same size ke dataframes can only be multiplied\n",
    "#toh .array into convert u have to multiply\n",
    "tfidf_scratch = tdm_len_scratch*idf.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f9645c-bb74-4c98-87c3-3d502da198d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b2a9068-c26c-48f1-9012-0ae793dea1c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#inbuilt tf-idf\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      3\u001b[0m tf1 \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "#inbuilt tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf1 = TfidfVectorizer()\n",
    "X2 = tf1.fit_transform(final)\n",
    "tfidf_inbuilt = pd.DataFrame(X2.toarray().T,index=tf1.get_feature_names(),columns=filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aa8fcf-c97d-4a1b-a024-a3abc211de62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_inbuilt\n",
    "#scratch and inbuilt not matching since hum smoothing kr dete hai...deno and sum mein inbuilt mein 1 add krte hai\n",
    "#u can finish smoothing by writing parameter smooth_idf=false in tdidfVectorizer\n",
    "#but still value nit matched since by default normalization l2 \n",
    "#aab iske baad bhi values  match nhi ho rhi\n",
    "#toh now u have challenge to match the values inbuilt and scratch wali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78020675-e91a-4d03-8fda-dbdd8870f196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TTM \n",
    "#Co-occurance\n",
    "#m*m size\n",
    "#kitni documents mein yeh terms ek saath occur ho rhi hai\n",
    "#binary tdm banakr oske transpose we multiply\n",
    "\n",
    "cooccur_matrix = np.dot(tdm_binary_scratch.values,tdm_binary_scratch.values.T)\n",
    "\n",
    "# '*' se multiply dataframe nhi hote.... .values se converted it into array and multiplied\n",
    "\n",
    "#ismein diagonal elements ka koi matlab nhi...like car with car...no meaning toh onko zero krlo\n",
    "np.fill_diagonal(cooccur_matrix,0)\n",
    "\n",
    "pd.DataFrame(cooccur_matrix,index=tdm_binary_scratch.index,columns=tdm_binary_scratch.index)\n",
    "\n",
    "#columns and index dono ko words given becaause term term\n",
    "# 0 of any of two terms means no document in which both occur together\n",
    "# 2 means 2 documents mein voh do terms ek saath\n",
    "#no built method for cooccurance method as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c836df2-9e3c-436d-91f3-1d5306bd446d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
